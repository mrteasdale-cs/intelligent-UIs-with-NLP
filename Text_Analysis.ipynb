{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-08T09:29:40.424684Z",
     "start_time": "2025-06-08T09:29:08.912873Z"
    }
   },
   "source": [
    "import re\n",
    "import nltk\n",
    "import textblob\n",
    "from bleach import clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ],
   "id": "9b9d15ef46f0f9b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T01:52:24.041138Z",
     "start_time": "2025-06-07T01:52:24.035414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocessing(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, flags=re.I|re.A)\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_tokens = []\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in stop_words:\n",
    "            clean_tokens.append(token)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i, token in enumerate(clean_tokens):\n",
    "        clean_tokens[i] = lemmatizer.lemmatize(token).lower().strip()\n",
    "\n",
    "    return ' '.join(clean_tokens)\n"
   ],
   "id": "b6a329091a4eed1e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T08:00:43.502909Z",
     "start_time": "2025-06-07T08:00:43.478917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_text = \"The film suffers from poor writing, with an underdeveloped plot and dragged-out, unnecessary dialogue that makes it feel unbearably long and boring; even the decent cinematography and occasional good performance can't save it from being a forgettable experience.\"\n",
    "print(\"===Sample Text===\")\n",
    "print(sample_text, '\\n')\n",
    "print(\"===Cleaned Text===\")\n",
    "print(preprocessing(sample_text))"
   ],
   "id": "fc877f989432e097",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Sample Text===\n",
      "The film suffers from poor writing, with an underdeveloped plot and dragged-out, unnecessary dialogue that makes it feel unbearably long and boring; even the decent cinematography and occasional good performance can't save it from being a forgettable experience. \n",
      "\n",
      "===Cleaned Text===\n",
      "['The', 'film', 'suffers', 'poor', 'writing', ',', 'underdeveloped', 'plot', 'dragged-out', ',', 'unnecessary', 'dialogue', 'makes', 'feel', 'unbearably', 'long', 'boring', ';', 'even', 'decent', 'cinematography', 'occasional', 'good', 'performance', 'ca', \"n't\", 'save', 'forgettable', 'experience', '.']\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2 Sentiment Analysis\n",
    "...is a technique that involves the process of determining and extracting sentiment or emotional information from text data. The primary goal of sentiment analysis is to assess and quantify the subjective aspects of a piece of text, typically in the form of opinions, attitudes, emotions, or polarity (positive, negative, or neutral)."
   ],
   "id": "55553d23e5389a88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T01:52:38.786615Z",
     "start_time": "2025-06-07T01:52:38.781618Z"
    }
   },
   "cell_type": "code",
   "source": "from textblob import TextBlob",
   "id": "5afde579302ff62f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T01:55:45.748559Z",
     "start_time": "2025-06-07T01:55:45.742561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = sample_text\n",
    "\n",
    "blob = TextBlob(input_text)\n",
    "\n",
    "# get sentiment using sentiment function\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment)\n",
    "\n",
    "# get polarity and subjectivity\n",
    "polarity = sentiment.polarity\n",
    "subjectivity = sentiment.subjectivity\n",
    "\n",
    "if polarity > 0:\n",
    "    sentiment_lbl = \"Positive\"\n",
    "elif polarity < 0:\n",
    "    sentiment_lbl = \"Negative\"\n",
    "else:\n",
    "    sentiment_lbl = \"Neutral\"\n",
    "\n",
    "# Print the results\n",
    "print(f\"Text: {input_text}\")\n",
    "print(f\"Sentiment: {sentiment_lbl}\")\n",
    "print(f\"Polarity: {polarity}\")\n",
    "print(f\"Subjectivity: {subjectivity}\")"
   ],
   "id": "cdf1d966e38b50c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.6678571428571428, subjectivity=0.6928571428571428)\n",
      "Text: Check out this amazing website: https://www.example.com. It has some great resources! ðŸ˜ƒ\n",
      "\n",
      "I can't believe it's already summer â˜€ï¸. Time flies when you're having fun!\n",
      "\n",
      "Did you see the latest news about the space mission to Mars? ðŸš€ It's truly fascinating.\n",
      "\n",
      "Don't forget to visit our website at https://www.example.com for more information. Have a great day! ðŸŒŸ\n",
      "Sentiment: Positive\n",
      "Polarity: 0.6678571428571428\n",
      "Subjectivity: 0.6928571428571428\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "82b37e2faa0b5073"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 3 NER\n",
    "Named Entity Recognition, or NER, is a fundamental technique in natural language processing. It automatically finds and classifies important items in text, such as names of people, organizations, places, dates, and monetary values. NER works by first identifying the boundaries of these entities and then assigning them to categories like PERSON, ORGANIZATION, or LOCATION.\n",
    "\n",
    "This process turns unstructured text into structured data, which is useful for tasks like extracting information, answering questions, and summarizing documents. For example, in the sentence â€œApple Inc. is opening a store in New York,â€ NER would identify â€œApple Inc.â€ as an organization and â€œNew Yorkâ€ as a location.\n",
    "\n",
    "We use the spaCy library for NER. With spaCy, you load a language model, process your text, and it automatically finds and labels named entities. You can then print out each entity and its label, such as â€˜PERSONâ€™ or â€˜ORGâ€™, to see what has been extracted and categorized. This makes it easy to pull structured information from large amounts of text.\n",
    "\n",
    "it can recognise such entities like persons, locations, orgs, dates, quantities, money, and more.\n",
    "it can be used for Information Extraction: Structuring unstructured text data for further analysis, Question Answering: Enhancing search engines to provide direct answers to questions, Language Translation: Improving the quality of translation by preserving named entities, Entity Linking: Associating recognised entities with external knowledge bases or databases, text summaries, and even news/social media analysis"
   ],
   "id": "841e242c5667ad2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# must download model first\n",
    "# python -m spacy download en_core_web_sm"
   ],
   "id": "2fa26e9114af8617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T03:59:38.146896Z",
     "start_time": "2025-06-07T03:59:37.239430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Named Entity Recognition\n",
    "import spacy\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Input text (replace this with your desired text)\n",
    "input_text = \"\"\"\n",
    "From the garage to the Googleplex.\n",
    "\n",
    "The Google story begins in 1995 at Stanford University. Larry Page was considering Stanford for grad school and Sergey Brin, a student there, was assigned to show him around. By some accounts, they disagreed about nearly everything during that first meeting, but by the following year they struck a partnership. Working from their dorm rooms, they built a search engine that used links to determine the importance of individual pages on the World Wide Web. They called this search engine Backrub. \n",
    "\n",
    "Soon after, Backrub was renamed Google (phew). The name was a play on the mathematical expression for the number 1 followed by 100 zeros and aptly reflected Larry and Sergey's mission to organise the worldâ€™s information and make it universally accessible and useful. Over the next few years, Google caught the attention of not only the academic community, but Silicon Valley investors as well. In August 1998, Sun co-founder Andy Bechtolsheim wrote Larry and Sergey a check for $100,000, and Google Inc. was officially born. With this investment, the newly incorporated team made the upgrade from the dorms to their first office: a garage in suburban Menlo Park, California, owned by Susan Wojcicki (employee no.16 and former CEO of YouTube). Clunky desktop computers, a ping pong table and bright blue carpet set the scene for those early days and late nights.(The tradition of keeping things colourful continues to this day.)\n",
    "\n",
    "Even in the beginning, things were unconventional: from Googleâ€™s initial server (made of Lego) to the first 'Doodle' in 1998: a stick figure in the logo announcing to site visitors that the\n",
    "entire staff was playing hooky at the Burning Man Festival. 'Don't be evil' captured the spirit of our intentionally unconventional methods. In the years that followed, the company expanded\n",
    "rapidly â€“ hiring engineers, building a sales team and introducing the first company dog, Yoshka. Google outgrew the garage and eventually moved to its current headquarters (aka'The Googleplex') in Mountain View, California. The spirit of doing things differently made the move. So did Yoshka.\n",
    "\n",
    "The relentless search for better answers continues to be at the core of everything we do. Today, Google makes hundreds of products used by billions of people across the globe,\n",
    " from YouTube and Android to Gmail and, of course, Google Search. Although weâ€™ve ditched the Lego servers and added just a few more company dogs, our passion for building technology\n",
    "for everyone has stayed with us â€“ from the dorm room to the garage and to this very day.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Extract named entities and their labels\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "print(doc)\n",
    "# Print the named entities and their labels\n",
    "for entity, label in entities:\n",
    "    print(f\"Entity: {entity}, Label: {label}\")"
   ],
   "id": "d9508770d37cbac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "From the garage to the Googleplex\n",
      "The Google story begins in 1995 at Stanford University. Larry Page was considering Stanford for grad school and Sergey Brin, a student there, was assigned to show him around. By some accounts, they disagreed about nearly everything during that first meeting, but by the following year they struck a partnership. Working from their dorm rooms, they built a search engine that used links to determine the importance of individual pages on the World Wide Web. They called this search engine Backrub. \n",
      "\n",
      "Soon after, Backrub was renamed Google (phew). The name was a play on the mathematical expression for the number 1 followed by 100 zeros and aptly reflected Larry and Sergey's mission to organise the worldâ€™s information and make it universally accessible and useful. Over the next few years, Google caught the attention of not only the academic community, but Silicon Valley investors as well. In August 1998, Sun co-founder Andy Bechtolsheim wrote Larry and Sergey a check for $100,000, and Google Inc. was officially born. With this investment, the newly incorporated team made the upgrade from the dorms to their first office: a garage in suburban Menlo Park, California, owned by Susan Wojcicki (employee no.16 and former CEO of YouTube). Clunky desktop computers, a ping pong table and bright blue carpet set the scene for those early days and late nights.(The tradition of keeping things colourful continues to this day.)\n",
      "\n",
      "Even in the beginning, things were unconventional: from Googleâ€™s initial server (made of Lego) to the first 'Doodle' in 1998: a stick figure in the logo announcing to site visitors that the\n",
      "entire staff was playing hooky at the Burning Man Festival. 'Don't be evil' captured the spirit of our intentionally unconventional methods. In the years that followed, the company expanded\n",
      "rapidly â€“ hiring engineers, building a sales team and introducing the first company dog, Yoshka. Google outgrew the garage and eventually moved to its current headquarters (aka'The Googleplex') in Mountain View, California. The spirit of doing things differently made the move. So did Yoshka.\n",
      "\n",
      "The relentless search for better answers continues to be at the core of everything we do. Today, Google makes hundreds of products used by billions of people across the globe,\n",
      " from YouTube and Android to Gmail and, of course, Google Search. Although weâ€™ve ditched the Lego servers and added just a few more company dogs, our passion for building technology\n",
      "for everyone has stayed with us â€“ from the dorm room to the garage and to this very day.\n",
      "\n",
      "Entity: Googleplex, Label: PERSON\n",
      "Entity: Google, Label: ORG\n",
      "Entity: 1995, Label: DATE\n",
      "Entity: Stanford University, Label: ORG\n",
      "Entity: Larry Page, Label: PERSON\n",
      "Entity: Stanford, Label: ORG\n",
      "Entity: Sergey Brin, Label: PERSON\n",
      "Entity: first, Label: ORDINAL\n",
      "Entity: the following year, Label: DATE\n",
      "Entity: the World Wide Web, Label: EVENT\n",
      "Entity: Backrub, Label: ORG\n",
      "Entity: Backrub, Label: ORG\n",
      "Entity: Google, Label: ORG\n",
      "Entity: 1, Label: CARDINAL\n",
      "Entity: 100 zeros, Label: CARDINAL\n",
      "Entity: Larry, Label: PERSON\n",
      "Entity: Sergey, Label: PERSON\n",
      "Entity: the next few years, Label: DATE\n",
      "Entity: Google, Label: ORG\n",
      "Entity: Silicon Valley, Label: LOC\n",
      "Entity: August 1998, Label: DATE\n",
      "Entity: Sun co-, Label: ORG\n",
      "Entity: Andy Bechtolsheim, Label: PERSON\n",
      "Entity: Larry, Label: PERSON\n",
      "Entity: Sergey, Label: PERSON\n",
      "Entity: 100,000, Label: MONEY\n",
      "Entity: Google Inc., Label: ORG\n",
      "Entity: first, Label: ORDINAL\n",
      "Entity: Menlo Park, Label: GPE\n",
      "Entity: California, Label: GPE\n",
      "Entity: Susan Wojcicki, Label: PERSON\n",
      "Entity: YouTube, Label: ORG\n",
      "Entity: those early days, Label: DATE\n",
      "Entity: this day, Label: DATE\n",
      "Entity: Google, Label: ORG\n",
      "Entity: Lego, Label: PERSON\n",
      "Entity: first, Label: ORDINAL\n",
      "Entity: 1998, Label: DATE\n",
      "Entity: the Burning Man Festival, Label: EVENT\n",
      "Entity: the years, Label: DATE\n",
      "Entity: first, Label: ORDINAL\n",
      "Entity: Yoshka, Label: PERSON\n",
      "Entity: Googleplex, Label: PERSON\n",
      "Entity: Mountain View, Label: GPE\n",
      "Entity: California, Label: GPE\n",
      "Entity: Yoshka, Label: PERSON\n",
      "Entity: Today, Label: DATE\n",
      "Entity: Google, Label: ORG\n",
      "Entity: hundreds, Label: CARDINAL\n",
      "Entity: billions, Label: CARDINAL\n",
      "Entity: YouTube, Label: ORG\n",
      "Entity: Android, Label: ORG\n",
      "Entity: Gmail, Label: PERSON\n",
      "Entity: Google Search, Label: ORG\n",
      "Entity: Lego, Label: ORG\n",
      "Entity: this very day, Label: DATE\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 4 Text Summarisation",
   "id": "b256189c5bde5a43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Text summarisation in Natural Language Processing (NLP) is the process of automatically generating a concise, coherent, and informative summary from a longer document, such as an article or report. The goal is to preserve the essential points and main ideas while omitting redundant or less relevant content.\n",
    "\n",
    "There are two main types of text summarisation:\n",
    "\n",
    "Extractive Summarisation: This approach selects and combines the most important sentences or phrases directly from the original text, typically using algorithms that score sentence importance. The summary consists of verbatim sections from the source, chosen to best represent the overall content.\n",
    "\n",
    "Abstractive Summarisation: This method generates new sentences that paraphrase the source material, using advanced natural language generation techniques. Abstractive summaries can capture the meaning of the original text in a more human-like and coherent manner, but are technically more challenging to implement.\n",
    "\n",
    "Applications of text summarisation include:\n",
    "- Information retrieval: Helping users quickly understand the content of lengthy documents or web pages.\n",
    "- News summarisation: Producing brief summaries of news articles for rapid consumption.\n",
    "- Document summarisation: Creating abstracts or executive summaries for research papers, business reports, or legal documents.\n",
    "- Content generation: Generating short descriptions for products, search results, or digital assistants."
   ],
   "id": "d356692e0f78543b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T03:59:44.620833Z",
     "start_time": "2025-06-07T03:59:44.614311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn. metrics.pairwise import cosine_similarity"
   ],
   "id": "c86a9a039a99436e",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T04:02:06.729515Z",
     "start_time": "2025-06-07T04:02:06.703280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_sentence(text):\n",
    "\n",
    "    # tokenize and remove stopwrods\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = nltk.word_tokenize(text)\n",
    "    cleaned_sentence = []\n",
    "    for i, token in enumerate(sentence):\n",
    "        if token not in stop_words:\n",
    "            cleaned_sentence.append(token)\n",
    "\n",
    "    return cleaned_sentence\n",
    "\n",
    "def build_similarity_matrix(sentences):\n",
    "    \"\"\"Build a similarity matrix for the sentences using TF-IDF\"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def summarise_text(text, max_sentences=6):\n",
    "    ranked_sentences = [] \n",
    "    summarised_text = \"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    preprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    similarity_matrix = build_similarity_matrix([\" \".join(sentence) for sentence in preprocessed_sentences])\n",
    "    \n",
    "    # generate a matrix of rnakings\n",
    "    sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # sort and rank top sentences\n",
    "    # ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    for i, s in enumerate(sentences):\n",
    "        ranked_sentences.append((scores[i], s.strip().replace(\"\\n\", \". \")))\n",
    "    sorted_ranked_sentences = sorted(ranked_sentences, reverse=True)\n",
    "    # join the ranked sentences to the summarised text, up to the max sentence provided parameter\n",
    "    for ranked_sentence in sorted_ranked_sentences[:max_sentences]:\n",
    "        summarised_text += \"\".join(ranked_sentence[1])\n",
    "    \n",
    "    return summarised_text\n",
    "\n",
    "summarise_text(input_text, 5)"
   ],
   "id": "7c3c28ee5c596bce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From the garage to the Googleplex. The Google story begins in 1995 at Stanford University.The spirit of doing things differently made the move.In the years that followed, the company expanded. rapidly â€“ hiring engineers, building a sales team and introducing the first company dog, Yoshka.The relentless search for better answers continues to be at the core of everything we do.Google outgrew the garage and eventually moved to its current headquarters (aka'The Googleplex') in Mountain View, California.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T03:31:13.775620Z",
     "start_time": "2025-06-07T03:31:13.764981Z"
    }
   },
   "cell_type": "code",
   "source": "preprocessed_text = preprocessing(input_text)",
   "id": "e707e4d4e484c19b",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T03:31:14.640829Z",
     "start_time": "2025-06-07T03:31:14.539586Z"
    }
   },
   "cell_type": "code",
   "source": "print(summarise_text(preprocessed_text, 4))",
   "id": "f592aa8fd3becfc4",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[48]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43msummarise_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocessed_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[45]\u001B[39m\u001B[32m, line 22\u001B[39m, in \u001B[36msummarise_text\u001B[39m\u001B[34m(text, max_sentences)\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msummarise_text\u001B[39m(text, max_sentences=\u001B[32m4\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     sentences = \u001B[43mnltk\u001B[49m\u001B[43m.\u001B[49m\u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m     preprocessed_sentences = [preprocessing(sentence) \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m sentences]\n\u001B[32m     25\u001B[39m     similarity_matrix = build_similarity_matrix([\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m.join(sentence) \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m sentences])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:120\u001B[39m, in \u001B[36msent_tokenize\u001B[39m\u001B[34m(text, language)\u001B[39m\n\u001B[32m    110\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[33;03mReturn a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[32m    112\u001B[39m \u001B[33;03musing NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    117\u001B[39m \u001B[33;03m:param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[32m    118\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    119\u001B[39m tokenizer = _get_punkt_tokenizer(language)\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1280\u001B[39m, in \u001B[36mPunktSentenceTokenizer.tokenize\u001B[39m\u001B[34m(self, text, realign_boundaries)\u001B[39m\n\u001B[32m   1276\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mTrue\u001B[39;00m) -> List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m   1277\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1278\u001B[39m \u001B[33;03m    Given a text, returns a list of the sentences in that text.\u001B[39;00m\n\u001B[32m   1279\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1280\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msentences_from_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1340\u001B[39m, in \u001B[36mPunktSentenceTokenizer.sentences_from_text\u001B[39m\u001B[34m(self, text, realign_boundaries)\u001B[39m\n\u001B[32m   1331\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msentences_from_text\u001B[39m(\n\u001B[32m   1332\u001B[39m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   1333\u001B[39m ) -> List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m   1334\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1335\u001B[39m \u001B[33;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[32m   1336\u001B[39m \u001B[33;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[32m   1337\u001B[39m \u001B[33;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[32m   1338\u001B[39m \u001B[33;03m    follows the period.\u001B[39;00m\n\u001B[32m   1339\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1340\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m:\u001B[49m\u001B[43me\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mspan_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1328\u001B[39m, in \u001B[36mPunktSentenceTokenizer.span_tokenize\u001B[39m\u001B[34m(self, text, realign_boundaries)\u001B[39m\n\u001B[32m   1326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m realign_boundaries:\n\u001B[32m   1327\u001B[39m     slices = \u001B[38;5;28mself\u001B[39m._realign_boundaries(text, slices)\n\u001B[32m-> \u001B[39m\u001B[32m1328\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msentence\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mslices\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1329\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentence\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1457\u001B[39m, in \u001B[36mPunktSentenceTokenizer._realign_boundaries\u001B[39m\u001B[34m(self, text, slices)\u001B[39m\n\u001B[32m   1444\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1445\u001B[39m \u001B[33;03mAttempts to realign punctuation that falls after the period but\u001B[39;00m\n\u001B[32m   1446\u001B[39m \u001B[33;03mshould otherwise be included in the same sentence.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1454\u001B[39m \u001B[33;03m    [\"(Sent1.)\", \"Sent2.\"].\u001B[39;00m\n\u001B[32m   1455\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1456\u001B[39m realign = \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1457\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msentence1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentence2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_pair_iter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslices\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1458\u001B[39m \u001B[43m    \u001B[49m\u001B[43msentence1\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mslice\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msentence1\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentence1\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1459\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msentence2\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001B[39m, in \u001B[36m_pair_iter\u001B[39m\u001B[34m(iterator)\u001B[39m\n\u001B[32m    319\u001B[39m iterator = \u001B[38;5;28miter\u001B[39m(iterator)\n\u001B[32m    320\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m321\u001B[39m     prev = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    322\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1429\u001B[39m, in \u001B[36mPunktSentenceTokenizer._slices_from_text\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m   1427\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_slices_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) -> Iterator[\u001B[38;5;28mslice\u001B[39m]:\n\u001B[32m   1428\u001B[39m     last_break = \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1429\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_match_potential_end_contexts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1430\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtext_contains_sentbreak\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1431\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mslice\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlast_break\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\desktopvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1394\u001B[39m, in \u001B[36mPunktSentenceTokenizer._match_potential_end_contexts\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m   1392\u001B[39m previous_slice = \u001B[38;5;28mslice\u001B[39m(\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m)\n\u001B[32m   1393\u001B[39m previous_match = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1394\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m match \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_lang_vars\u001B[49m\u001B[43m.\u001B[49m\u001B[43mperiod_context_re\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfinditer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   1395\u001B[39m     \u001B[38;5;66;03m# Get the slice of the previous word\u001B[39;00m\n\u001B[32m   1396\u001B[39m     before_text = text[previous_slice.stop : match.start()]\n\u001B[32m   1397\u001B[39m     index_after_last_space = \u001B[38;5;28mself\u001B[39m._get_last_whitespace_index(before_text)\n",
      "\u001B[31mTypeError\u001B[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "execution_count": 48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
